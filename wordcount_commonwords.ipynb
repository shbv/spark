{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import Row, StructField, StructType, StringType, IntegerType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create SparkContext object with 4 executors. It is the delegate between head and executors\n",
    "sc = SparkContext(master=\"local[4]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SparkContext master=local[4] appName=pyspark-shell>\n"
     ]
    }
   ],
   "source": [
    "print(sc)\n",
    "#sc.stop() ; # to stop context. Do this before starting new one. Only 1 at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.1'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Download from \n",
    "url = \"https://mas-dse-open.s3.amazonaws.com/Moby-Dick.txt\"\n",
    "# to\n",
    "data_dir = './data/'\n",
    "data_file = './data/Moby-Dick.txt'"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# option 1\n",
    "import urllib.request, urllib.parse, urllib.error\n",
    "f = urllib.request.urlretrieve (url, data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/Moby-Dick.txt exists\n",
      "-rw-rw-r-- 1 sb sb 1.2M Apr  6  2016 ./data/Moby-Dick.txt\r\n"
     ]
    }
   ],
   "source": [
    "# option 2\n",
    "from os.path import split,join,exists\n",
    "\n",
    "if exists(data_file):\n",
    "    print(data_file,\"exists\")\n",
    "else:\n",
    "    command=\"wget %s -P %s \"%(url, data_dir)\n",
    "    print(command)\n",
    "    !$command\n",
    "!ls -lh $data_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Open file\n",
    "txt_rdd = sc.textFile(data_file)\n",
    "type(txt_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12 ms, sys: 0 ns, total: 12 ms\n",
      "Wall time: 63.5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Split text to words, remove empty words, count using reducebykey\n",
    "words = txt_rdd.flatMap(lambda txt: txt.split(\" \"))\\\n",
    "                .filter(lambda word: word!='')\n",
    "word_count = words.map(lambda word: (word,1))\\\n",
    "                    .reduceByKey(lambda cnt1, cnt2: cnt1+cnt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['(2) PythonRDD[6] at RDD at PythonRDD.scala:48 []',\n",
       " ' |  ./data/Moby-Dick.txt MapPartitionsRDD[1] at textFile at NativeMethodAccessorImpl.java:0 []',\n",
       " ' |  ./data/Moby-Dick.txt HadoopRDD[0] at textFile at NativeMethodAccessorImpl.java:0 []']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.toDebugString().decode().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['(2) PythonRDD[7] at RDD at PythonRDD.scala:48 []',\n",
       " ' |  MapPartitionsRDD[5] at mapPartitions at PythonRDD.scala:436 []',\n",
       " ' |  ShuffledRDD[4] at partitionBy at NativeMethodAccessorImpl.java:0 []',\n",
       " ' +-(2) PairwiseRDD[3] at reduceByKey at <timed exec>:4 []',\n",
       " '    |  PythonRDD[2] at reduceByKey at <timed exec>:4 []',\n",
       " '    |  ./data/Moby-Dick.txt MapPartitionsRDD[1] at textFile at NativeMethodAccessorImpl.java:0 []',\n",
       " '    |  ./data/Moby-Dick.txt HadoopRDD[0] at textFile at NativeMethodAccessorImpl.java:0 []']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_count.toDebugString().decode().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 ms, sys: 4 ms, total: 8 ms\n",
      "Wall time: 767 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "33781"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Number of different words\n",
    "word_count.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 4 ms, total: 4 ms\n",
      "Wall time: 54.9 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "215133"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Total number of words\n",
    "word_count.map(lambda wordcnt: wordcnt[1])\\\n",
    "            .reduce(lambda cnt1, cnt2: cnt1+cnt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 ms, sys: 0 ns, total: 4 ms\n",
      "Wall time: 52.8 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "215133"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Total number of words - again to check if anything is cached\n",
    "word_count.map(lambda wordcnt: wordcnt[1])\\\n",
    "            .reduce(lambda cnt1, cnt2: cnt1+cnt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 ms, sys: 4 ms, total: 8 ms\n",
      "Wall time: 102 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "215133"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Total number of words - again with explicit cache\n",
    "word_count.cache().map(lambda wordcnt: wordcnt[1])\\\n",
    "            .reduce(lambda cnt1, cnt2: cnt1+cnt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 ms, sys: 0 ns, total: 4 ms\n",
      "Wall time: 46 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "215133"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Total number of words - again with explicit cache used\n",
    "word_count.map(lambda wordcnt: wordcnt[1])\\\n",
    "            .reduce(lambda cnt1, cnt2: cnt1+cnt2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of words: 33781\n",
      "common words:\n",
      "to: 4510\n",
      "a: 4533\n",
      "and: 5951\n",
      "of: 6587\n",
      "the: 13766\n",
      "CPU times: user 28 ms, sys: 4 ms, total: 32 ms\n",
      "Wall time: 56.8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Option 1: Python way: Collect and compute at head node\n",
    "\n",
    "txt = word_count.collect()\n",
    "print(\"# of words:\",len(txt))\n",
    "txt.sort(key = lambda word_cnt: word_cnt[1])\n",
    "prnt = \"\\n\".join(['%s: %d'%wcpair for wcpair in txt[-5:]])\n",
    "print(\"common words:\\n\" + prnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2) PythonRDD[19] at RDD at PythonRDD.scala:48 []\n",
      " |  MapPartitionsRDD[18] at mapPartitions at PythonRDD.scala:436 []\n",
      " |  ShuffledRDD[17] at partitionBy at NativeMethodAccessorImpl.java:0 []\n",
      " +-(2) PairwiseRDD[16] at sortByKey at <timed exec>:6 []\n",
      "    |  PythonRDD[15] at sortByKey at <timed exec>:6 []\n",
      "    |  PythonRDD[7] at RDD at PythonRDD.scala:48 []\n",
      "    |      CachedPartitions: 2; MemorySize: 630.8 KB; ExternalBlockStoreSize: 0.0 B; DiskSize: 0.0 B\n",
      "    |  MapPartitionsRDD[5] at mapPartitions at PythonRDD.scala:436 []\n",
      "    |  ShuffledRDD[4] at partitionBy at NativeMethodAccessorImpl.java:0 []\n",
      "    +-(2) PairwiseRDD[3] at reduceByKey at <timed exec>:4 []\n",
      "       |  PythonRDD[2] at reduceByKey at <timed exec>:4 []\n",
      "       |  ./data/Moby-Dick.txt MapPartitionsRDD[1] at textFile at NativeMethodAccessorImpl.java:0 []\n",
      "       |  ./data/Moby-Dick.txt HadoopRDD[0] at textFile at NativeMethodAccessorImpl.java:0 []\n",
      "common words:\n",
      "13766: the\n",
      "6587: of\n",
      "5951: and\n",
      "4533: a\n",
      "4510: to\n",
      "CPU times: user 16 ms, sys: 8 ms, total: 24 ms\n",
      "Wall time: 163 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Option 2: Spark way: Reverse word_count to count_word and sortbykey\n",
    "\n",
    "# Sort\n",
    "count_word = word_count.map(lambda wordcnt: (wordcnt[1],wordcnt[0]))\n",
    "count_word_sorted = count_word.sortByKey(ascending=False)\n",
    "\n",
    "# Execution plan\n",
    "[print(x) for x in count_word_sorted.toDebugString().decode().split('\\n')]\n",
    "\n",
    "# Execute\n",
    "txt = count_word_sorted.take(10)\n",
    "prnt = \"\\n\".join(['%d: %s'%wcpair for wcpair in txt[:5]])\n",
    "print(\"common words:\\n\" + prnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
