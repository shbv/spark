{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import Row, StructField, StructType, StringType, IntegerType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create SparkContext object with 4 executors. It is the delegate between head and executors\n",
    "sc = SparkContext(master=\"local[4]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SparkContext master=local[4] appName=pyspark-shell>\n"
     ]
    }
   ],
   "source": [
    "print(sc)\n",
    "#sc.stop() ; # to stop context. Do this before starting new one. Only 1 at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.1'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RDDs (Resilient distributed datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Main data structure in Spark.\n",
    "# Driver node runs this program. \n",
    "#    It has pointer to object that has locations of RDD elements (partitions)\n",
    "# The elements (partitions) of RDD are distributed on worker nodes.  \n",
    "#    Executors on worker nodes work on them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PythonRDD[1] at RDD at PythonRDD.scala:48\n",
      "[0, 1, 2] <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "# Create RDD\n",
    "x = range(3)\n",
    "rdd = sc.parallelize(x)\n",
    "print(rdd)\n",
    "\n",
    "# Collect RDD on head node\n",
    "y = rdd.collect()\n",
    "print(y, type(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Map Reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 4]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Map \n",
    "# 1:1. \n",
    "# Operation function is parameter. \n",
    "# All operations in parallel on data local to it\n",
    "out = rdd.map(lambda x: x*x)\n",
    "out.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "## Reduce \n",
    "# many:1. \n",
    "# Operation function is input\n",
    "# Repeatedly does reduce on data local to it.\n",
    "# Operation function should be commutative (order independent) \n",
    "#   reduce is like a tree with result at root.\n",
    "out = rdd.reduce(lambda x,y: x+y)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "this\n"
     ]
    }
   ],
   "source": [
    "## Reduce - more complex:\n",
    "\n",
    "words = [\"this\", \"is\", \"a\", \"test\"]\n",
    "\n",
    "# 1: find min length word\n",
    "rdd2= sc.parallelize(words)\n",
    "out = rdd2.reduce(lambda x,y: x if len(x)<len(y) else y)\n",
    "print(out)\n",
    "\n",
    "# 2: find last lexicograph word among max length words\n",
    "rdd3 = sc.parallelize(words)\n",
    "def last_lex_word(x,y):\n",
    "    if len(x) > len(y):\n",
    "        return x\n",
    "    elif len(y) > len(x):\n",
    "        return y\n",
    "    else:\n",
    "        if x > y: return x\n",
    "        else: return y\n",
    "out = rdd3.reduce(last_lex_word)\n",
    "print(out)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Number of workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SparkContext master=local[4] appName=pyspark-shell>\n"
     ]
    }
   ],
   "source": [
    "print(sc)\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [10, 11] * 100000\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of workers:  1, time taken: 18.14\n",
      "num of workers:  2, time taken: 5.56\n",
      "num of workers:  3, time taken: 3.16\n",
      "num of workers:  4, time taken: 2.27\n",
      "num of workers:  5, time taken: 2.33\n",
      "num of workers:  6, time taken: 2.10\n",
      "num of workers:  7, time taken: 1.97\n",
      "num of workers:  8, time taken: 1.83\n",
      "num of workers:  9, time taken: 1.95\n",
      "num of workers: 10, time taken: 1.98\n",
      "num of workers: 11, time taken: 2.15\n",
      "num of workers: 12, time taken: 2.19\n",
      "num of workers: 13, time taken: 2.14\n",
      "num of workers: 14, time taken: 2.20\n",
      "num of workers: 15, time taken: 2.06\n"
     ]
    }
   ],
   "source": [
    "# Recommendation is 1 per core . \n",
    "# It improves till 8 here and gets worse after that (hyperthreading with 4 cores => 8)\n",
    "\n",
    "from time import time\n",
    "for num_workers in range(1,16):\n",
    "    sc = SparkContext(master = \"local[%d]\" % num_workers)\n",
    "    tic=time()\n",
    "    # some dummy operation\n",
    "    for dummy_itn in range(10):\n",
    "        sc.parallelize(data).reduce(lambda x,y: x*y)\n",
    "    exec_time = time() - tic\n",
    "    print(\"num of workers: %2d, time taken: %3.2f\" %(num_workers,exec_time))\n",
    "    sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Lazy evaluation (pipelined execution), Execution plan (dependence graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Computation starts only when result is needed. (not when command is executed).\n",
    "# Map/reduce only constructs the execution plan\n",
    "# Minimizes number of memory accesses (and cache misses) and single pass through data. \n",
    "# Also, Smaller memory footprint since not intermediate results are stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from math import sin\n",
    "def consume_time(i):\n",
    "    [sin(i) for i in range(50)]\n",
    "    return sin(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 ms, sys: 0 ns, total: 4 ms\n",
      "Wall time: 23.6 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8414709848078965"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "consume_time(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# create spark context\n",
    "sc = SparkContext(master = \"local[4]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 ms, sys: 0 ns, total: 4 ms\n",
      "Wall time: 3.22 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# create rdd\n",
    "data = range(100000)\n",
    "rdd = sc.parallelize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4) PythonRDD[1] at RDD at PythonRDD.scala:48 []\n",
      " |  ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:489 []\n"
     ]
    }
   ],
   "source": [
    "# View execution plan, as dependence graph\n",
    "# Top-most line is output. Bottom line is input rdd \n",
    "print(rdd.toDebugString().decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 21.7 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Operation but no execution (map doesnt need to output anything)\n",
    "temp_rdd = rdd.map(lambda x: consume_time(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4) PythonRDD[2] at RDD at PythonRDD.scala:48 []\n",
      " |  ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:489 []\n"
     ]
    }
   ],
   "source": [
    "# View execution plan, as dependence graph\n",
    "# 1stTop-most line is output. Bottom line is input rdd \n",
    "print(temp_rdd.toDebugString().decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 ms, sys: 4 ms, total: 8 ms\n",
      "Wall time: 464 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Operation reduce has to print output. \n",
    "# temp_rdd map is evaluated now\n",
    "out = temp_rdd.reduce(lambda x,y: x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8 ms, sys: 0 ns, total: 8 ms\n",
      "Wall time: 174 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "# Another operation with same temp_rdd\n",
    "# temp_rdd map is evaluated again instead of re-using it. \n",
    "# So, runtime doesnt reduce.\n",
    "out = temp_rdd.filter(lambda x: x>0).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 ms, sys: 0 ns, total: 4 ms\n",
      "Wall time: 8.02 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Add plan to cache the intermediate result (memory vs runtime tradeoff)\n",
    "temp_rdd = rdd.map(lambda x: consume_time(x)).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4) PythonRDD[5] at RDD at PythonRDD.scala:48 [Memory Serialized 1x Replicated]\n",
      " |  ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:489 [Memory Serialized 1x Replicated]\n"
     ]
    }
   ],
   "source": [
    "# View execution plan, as dependence graph\n",
    "# Top-most line is output. Bottom line is input rdd \n",
    "print(temp_rdd.toDebugString().decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 ms, sys: 0 ns, total: 4 ms\n",
      "Wall time: 239 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Operation reduce has to print output. \n",
    "# temp_rdd map is evaluated now and cached\n",
    "out = temp_rdd.reduce(lambda x,y: x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 4 ms, total: 4 ms\n",
      "Wall time: 67.7 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "# Another operation with same temp_rdd\n",
    "# temp_rdd map cache re-used. \n",
    "# So, runtime doesnt reduce.\n",
    "out = temp_rdd.filter(lambda x: x>0).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Partitions:\n",
    "# Number of partitions of the RDD\n",
    "# Default: Number of workers in spark context\n",
    "# After some RDD operations, some worker partitions may not have local data and will be idle.\n",
    "# Can repartition for load balancing\n",
    "# Should be atleast number of workers so that executor in each worker has a unit to work on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "# Initial number of partitions\n",
    "rdd1 = sc.parallelize(range(100))\n",
    "print(rdd1.getNumPartitions())\n",
    "\n",
    "rdd2 = sc.parallelize(range(100), numSlices = 8)\n",
    "print(rdd2.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Gloming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12, 13, 12, 13, 12, 13, 12, 13]\n"
     ]
    }
   ],
   "source": [
    "# Used to refer to elements of RDD partition\n",
    "# Transforms partition into a tuple (immutable list)\n",
    "\n",
    "# Print length of each partition\n",
    "print(rdd2.glom().map(len).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], 12),\n",
       " (12, [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24], 13),\n",
       " (25, [25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36], 12),\n",
       " (37, [37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49], 13),\n",
       " (50, [50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61], 12),\n",
       " (62, [62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74], 13),\n",
       " (75, [75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86], 12),\n",
       " (87, [87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99], 13)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print elements in each glom tuple\n",
    "def getpartinfo(g, print_level = 1):\n",
    "    if len(g) > 0:\n",
    "        if print_level == 1:\n",
    "            out = (g[0], g, len(g))\n",
    "        else:\n",
    "            out = len(g)\n",
    "    else:\n",
    "        out = None\n",
    "    return(out)\n",
    "rdd2.glom().map(lambda g: getpartinfo(g)).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Editing partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "rdd1 = sc.parallelize(range(100), numSlices = 10)\n",
    "print(rdd1.getNumPartitions())\n",
    "rdd2 = rdd1.map(lambda x: (x,x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((0, 0), 10),\n",
       " ((10, 10), 10),\n",
       " ((20, 20), 10),\n",
       " ((30, 30), 10),\n",
       " ((40, 40), 10),\n",
       " ((50, 50), 10),\n",
       " ((60, 60), 10),\n",
       " ((70, 70), 10),\n",
       " ((80, 80), 10),\n",
       " ((90, 90), 10)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print elements in each glom tuple\n",
    "def getpartinfo(g, print_level = 1):\n",
    "    if len(g) > 0:\n",
    "        if print_level == 1:\n",
    "            out = (g[0], len(g))\n",
    "        elif print_level == 2:\n",
    "            out = (g[0][0], g, len(g))        \n",
    "        else:\n",
    "            out = len(g)\n",
    "    else:\n",
    "        out = None\n",
    "    return(out)\n",
    "rdd2.glom().map(lambda g: getpartinfo(g,1)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[((90, 90), 10),\n",
       " ((0, 0), 10),\n",
       " ((60, 60), 10),\n",
       " ((40, 40), 10),\n",
       " None,\n",
       " ((30, 30), 20),\n",
       " ((10, 10), 10),\n",
       " ((70, 70), 10),\n",
       " ((50, 50), 10),\n",
       " ((20, 20), 10)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Option 1 to edit: Random partition\n",
    "# Give number of partitions\n",
    "# Easy but no control on where elements go\n",
    "\n",
    "rdd_temp = rdd2.repartition(10)\n",
    "print(rdd_temp.getNumPartitions())\n",
    "rdd_temp.glom().map(lambda g: getpartinfo(g)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[((0, 0), 10),\n",
       " ((1, 1), 10),\n",
       " ((2, 2), 10),\n",
       " ((3, 3), 10),\n",
       " ((4, 4), 10),\n",
       " ((5, 5), 10),\n",
       " ((6, 6), 10),\n",
       " ((7, 7), 10),\n",
       " ((8, 8), 10),\n",
       " ((9, 9), 10)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Option 2 to edit: Partition by key\n",
    "# Needs (key, value) element RDD with key as integer.\n",
    "# Give k. key%k is the partition# for each element.\n",
    "# Extra work to define key but more control\n",
    "\n",
    "rdd_temp = rdd2.partitionBy(10)\n",
    "print(rdd_temp.getNumPartitions())\n",
    "rdd_temp.glom().map(lambda g: getpartinfo(g, 1)).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### RDD Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Chaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1 = sc.parallelize([1,2,3])\n",
    "rdd1.map(lambda x: x*x*x)\\\n",
    "    .reduce(lambda x,y: x+y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### RDD get info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "rdd1 = sc.parallelize(range(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, [0, 1, 2, 3, 4])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.first(), rdd1.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8, 25, 33, 38, 62, 70]\n",
      "[7, 11, 59, 67, 91, 95, 99]\n"
     ]
    }
   ],
   "source": [
    "print(rdd1.sample(False, 5./100).collect())\n",
    "print(rdd1.sample(False, 5./100).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Transformations:  RDD -> RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Transforms RDD -> RDD. \n",
    "# No communication across partitions needed.\n",
    "# Examples: map, filter, flatMap, set operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "map: [['test1', 'a', 'b'], ['test2', 'c', 'd']]\n",
      "flatmap: ['test1', 'a', 'b', 'test2', 'c', 'd']\n",
      "100\n",
      "80\n"
     ]
    }
   ],
   "source": [
    "# map, flatMap, filter\n",
    "\n",
    "# map\n",
    "rdd1 = sc.parallelize([\"test1 a b\", \"test2 c d\"])\n",
    "print(\"map:\", rdd1.map(lambda x: x.split(\" \")).collect())\n",
    "\n",
    "# flatMap - single list\n",
    "print(\"flatmap:\", rdd1.flatMap(lambda x: x.split(\" \")).collect())\n",
    "\n",
    "# filter \n",
    "rdd1 = sc.parallelize(range(100))\n",
    "print(rdd1.count())\n",
    "print(rdd1.filter(lambda x: x >= 20).count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rdd1 [1, 1, 2, 5, 6]\n",
      "rdd2 [1, 2, 4, 5]\n",
      "union [1, 1, 2, 5, 6, 1, 2, 4, 5]\n",
      "intersection [1, 2, 5]\n",
      "rdd1 minus rdd2 [6]\n",
      "rdd1 cartesian rdd2 [(1, 1), (1, 2), (1, 4), (1, 5), (1, 1), (1, 2), (1, 4), (1, 5), (2, 1), (2, 2), (2, 4), (2, 5), (5, 1), (6, 1), (5, 2), (6, 2), (5, 4), (6, 4), (5, 5), (6, 5)]\n"
     ]
    }
   ],
   "source": [
    "# Set operations \n",
    "# Distinct set, union, intersection, subtract, cartesian\n",
    "\n",
    "rdd1 = sc.parallelize([1,1,2,5,6])\n",
    "rdd2 = sc.parallelize([1,2,4,5])\n",
    "print(\"rdd1\", rdd1.collect())\n",
    "print(\"rdd2\", rdd2.collect())\n",
    "\n",
    "print(\"union\", rdd1.union(rdd2).collect())\n",
    "print(\"intersection\", rdd1.intersection(rdd2).collect())\n",
    "print(\"rdd1 minus rdd2\", rdd1.subtract(rdd2).collect())\n",
    "print(\"rdd1 cartesian rdd2\", rdd1.cartesian(rdd2).collect())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Shuffle is also type of transformation RDD -> RDD\n",
    "# Lot of communication across partitions - expensive\n",
    "# Examples: distinct, sort, sortbykey, reducebykey, join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rdd1 distinct set [1, 5, 2, 6]\n"
     ]
    }
   ],
   "source": [
    "rdd1 = sc.parallelize([1,1,2,5,6])\n",
    "print(\"rdd1 distinct set\", rdd1.distinct().collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Actions:  RDD -> Python object on head node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Tranforms RDD to python object on head node\n",
    "# Some communication needed\n",
    "# Examples: reduce, count, collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1 = sc.parallelize([1,2,3])\n",
    "rdd1.map(lambda x: x*x*x)\\\n",
    "    .reduce(lambda x,y: x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### (key, value) Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3, 9), (1, 2), (2, 4), (2, 10)]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create (key,value) RDD\n",
    "data = [(3,9), (1,2), (2,4), (2,10)]\n",
    "rdd1 = sc.parallelize(data)\n",
    "rdd1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 2), (2, 14), (3, 9)]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reducebykey\n",
    "rdd1.reduceByKey(lambda v1,v2: v1 + v2).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 2), (2, 4), (2, 10), (3, 9)]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sortbykey\n",
    "rdd1.sortByKey().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3, 12), (1, 5), (2, 7), (2, 13)]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mapvalues\n",
    "rdd1.mapValues(lambda v: v+3).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, <pyspark.resultiterable.ResultIterable object at 0x7f61057669b0>), (2, <pyspark.resultiterable.ResultIterable object at 0x7f61057668d0>), (3, <pyspark.resultiterable.ResultIterable object at 0x7f6105766e48>)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(1, [2]), (2, [4, 10]), (3, [9])]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# groupbykey\n",
    "print(rdd1.groupByKey().collect())\n",
    "rdd1.groupByKey().mapValues(lambda iterable: [x for x in iterable]).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3, [9, 12]), (1, [2, 5]), (2, [4, 7]), (2, [10, 13])]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(3, 9), (3, 12), (1, 2), (1, 5), (2, 4), (2, 7), (2, 10), (2, 13)]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# flatmapvalues\n",
    "print(rdd1.mapValues(lambda v: [v,v+3]).collect())\n",
    "rdd1.flatMapValues(lambda v: [v,v+3]).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('t1', 1), ('t1', 2), ('t1', 3), ('t1', 4), ('t2', 1), ('t2', 2), ('t2', 3), ('t3', 1), ('t3', 2), ('t3', 3)]\n",
      "[('t1', (10, 4)), ('t2', (6, 3)), ('t3', (6, 3))]\n",
      "[('t1', 2.5), ('t2', 2.0), ('t3', 2.0)]\n"
     ]
    }
   ],
   "source": [
    "# combinebykey\n",
    "\n",
    "data = [(\"t1\",1), (\"t1\",2), (\"t1\",3), (\"t1\", 4), (\"t2\",1), (\"t2\",2), (\"t2\",3), (\"t3\",1), (\"t3\",2), (\"t3\",3)]\n",
    "rdd1 = sc.parallelize(data)\n",
    "print(rdd1.collect())\n",
    "\n",
    "# sum of values per key\n",
    "rdd2 = rdd1.combineByKey(\n",
    "             # createCombiner() -> creates combiner or accumulator: (acc[0], acc[1]) here   \n",
    "            (lambda value: (value,1)), \n",
    "            # mergeValue() -> merge new value with existing accumulator acc\n",
    "            (lambda acc, value: (acc[0]+value,acc[1]+1)), \n",
    "            # mergeCombiner() -> merges accumulators by key across partitions\n",
    "            (lambda acc1, acc2: (acc1[0]+acc2[0], acc1[1]+acc2[1])) \n",
    "        )\n",
    "print(rdd2.collect())\n",
    "\n",
    "# avg of values per key\n",
    "rdd3 = rdd2.mapValues(lambda v: v[0]/v[1])\n",
    "print(rdd3.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3, 6)]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# subtractbykey\n",
    "rdd1 = sc.parallelize([(1,2), (1,3), (2,4)])\n",
    "rdd2 = sc.parallelize([(1,7), (3,6)])\n",
    "rdd2.subtractByKey(rdd1).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 2), (1, 3), (2, 4)]\n",
      "[(1, 7), (3, 6)]\n",
      "[(1, (2, 7)), (1, (3, 7))]\n",
      "[(1, (2, 7)), (1, (3, 7)), (2, (4, None))]\n",
      "[(1, (2, 7)), (1, (3, 7)), (3, (None, 6))]\n",
      "[(1, (2, 7)), (1, (3, 7)), (2, (4, None)), (3, (None, 6))]\n"
     ]
    }
   ],
   "source": [
    "# join\n",
    "rdd1 = sc.parallelize([(1,2), (1,3), (2,4)])\n",
    "rdd2 = sc.parallelize([(1,7), (3,6)])\n",
    "print(rdd1.collect())\n",
    "print(rdd2.collect())\n",
    "\n",
    "# default is inner join\n",
    "print(rdd1.join(rdd2).collect())\n",
    "\n",
    "# leftOuterJoin\n",
    "print(rdd1.leftOuterJoin(rdd2).collect())\n",
    "\n",
    "# rightOuterJoin\n",
    "print(rdd1.rightOuterJoin(rdd2).collect())\n",
    "\n",
    "# fullOuterJoin\n",
    "print(rdd1.fullOuterJoin(rdd2).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### (key, value) Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data = [(3,9), (1,2), (2,4), (2,10)]\n",
    "rdd1 = sc.parallelize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {3: 1, 1: 1, 2: 2})\n",
      "1 2\n"
     ]
    }
   ],
   "source": [
    "# count by key\n",
    "x = rdd1.countByKey()\n",
    "print(x)\n",
    "print(x[3], x[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{3: 9, 1: 2, 2: 10}\n",
      "9 10\n"
     ]
    }
   ],
   "source": [
    "# collect as dict\n",
    "x = rdd1.collectAsMap()\n",
    "print(x)\n",
    "print(x[3], x[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 10]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# search key\n",
    "rdd1.lookup(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Dataframe is an RDD of rows plus schema information\n",
    "# It is a restricted subtype of RDD\n",
    "# 2D structure - row is record, columns can be of different datatype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Create SQLContext similar to SparkContext to use sql\n",
    "sqc = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('A', 1), ('B', 2), ('C', 3)]\n",
      "root\n",
      " |-- name: string (nullable = false)\n",
      " |-- number: integer (nullable = false)\n",
      "\n",
      "[Row(name='A', number=1), Row(name='B', number=2), Row(name='C', number=3)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(pyspark.rdd.RDD, pyspark.sql.dataframe.DataFrame)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Option 1 to create dataframe: create schema and RDD of tuples\n",
    "\n",
    "# RDD\n",
    "data = [(\"A\", 1), (\"B\", 2), (\"C\", 3)]\n",
    "rdd = sc.parallelize(data)\n",
    "print(rdd.collect())\n",
    "\n",
    "# Schema\n",
    "sch = [StructField(\"name\", StringType(), False),\n",
    "       StructField(\"number\", IntegerType(), False)]\n",
    "schema = StructType(sch)\n",
    "\n",
    "# DF\n",
    "df = sqc.createDataFrame(rdd, schema)\n",
    "df.printSchema()\n",
    "print(df.collect())\n",
    "\n",
    "# Types:\n",
    "type(rdd), type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(name='A', number=1), Row(name='B', number=2), Row(name='C', number=3)]\n",
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- number: long (nullable = true)\n",
      "\n",
      "[Row(name='A', number=1), Row(name='B', number=2), Row(name='C', number=3)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(pyspark.rdd.RDD, pyspark.sql.dataframe.DataFrame)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Option 2 to create dataframe: create RDD of Rows\n",
    "\n",
    "# RDD\n",
    "data = [Row(name=\"A\", number=1), \n",
    "        Row(name=\"B\", number=2), \n",
    "        Row(name=\"C\", number=3)]\n",
    "rdd = sc.parallelize(data)\n",
    "print(rdd.collect())\n",
    "\n",
    "# DF\n",
    "df = sqc.createDataFrame(rdd)\n",
    "df.printSchema()\n",
    "print(df.collect())\n",
    "\n",
    "# Types:\n",
    "type(rdd), type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and saving files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### JSON format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/people.json\r\n"
     ]
    }
   ],
   "source": [
    "!ls data/people.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n",
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n",
      "+----+\n",
      "| age|\n",
      "+----+\n",
      "|null|\n",
      "|  30|\n",
      "|  19|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fpath = 'data/people.json'\n",
    "df = sqc.read.json(fpath)\n",
    "print(type(df))\n",
    "df.show()\n",
    "df.printSchema()\n",
    "df.select('age').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parquet format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/users.parquet\r\n"
     ]
    }
   ],
   "source": [
    "!ls data/users.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "+------+--------------+----------------+\n",
      "|  name|favorite_color|favorite_numbers|\n",
      "+------+--------------+----------------+\n",
      "|Alyssa|          null|  [3, 9, 15, 20]|\n",
      "|   Ben|           red|              []|\n",
      "+------+--------------+----------------+\n",
      "\n",
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- favorite_color: string (nullable = true)\n",
      " |-- favorite_numbers: array (nullable = true)\n",
      " |    |-- element: integer (containsNull = true)\n",
      "\n",
      "+------+--------------+\n",
      "|  name|favorite_color|\n",
      "+------+--------------+\n",
      "|Alyssa|          null|\n",
      "|   Ben|           red|\n",
      "+------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fpath = 'data/users.parquet'\n",
    "df = sqc.read.load(fpath)\n",
    "print(type(df))\n",
    "df.show()\n",
    "df.printSchema()\n",
    "df.select('name','favorite_color').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "part-00000-40f50b7a-e84a-4f5c-849c-46e8f55630b1-c000.snappy.parquet  _SUCCESS\r\n"
     ]
    }
   ],
   "source": [
    "df1 = df.select('name','favorite_color')\n",
    "df1.write.save('data/temp.parquet', mode='overwrite')\n",
    "!ls data/temp.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
